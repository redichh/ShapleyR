---
title: "shapley"
author: "Vignette Author"
date: '`r Sys.Date()`'
output:
  pdf_document: default
  html_document: default
vignette: |
  %\VignetteIndexEntry{shapley} %\VignetteEngine{knitr::rmarkdown} %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE,results='hide'}
library(ggplot2)
#library(shapleyr)
#load the packages that we need
devtools::load_all()
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(collapse = T, comment = "#>")
```

## R Shapley

The Shapley value is a method that gives a solution to the following problem: A coalition of players play a game, for which they get a payout, but it is not clear how to distribute the payout fairly among the players. The Shapley value solves this problem by trying out different coalitions of players to decide how much each player changes the amount of the payout. 
What does this have to do with machine learning? In machine learning, the features (=players) work together to get the payout (=predicted value). The Shapley value tells us, how much each feature contributed to the prediction.


Properties:
  * Efficiency.
  * Symmetry.
  * Linearity.
  * Zero player.

------------- -------------- --------------- -----------------
   v({ }) = 0    v({a}) = 12      v({b}) = 6        v({c}) = 9

v({a,b}) = 24  v({a,c}) = 27   v({b,c}) = 15   v({a,b,c}) = 36
------------- -------------- --------------- -----------------

For example  feature b:

Permutationen        Beitrag vor b     vor und mit b    Marginaler Beitrag
-------------       --------------     --------------   -------------------
        a,b,c       v({a}) = 12         v({a,b})=24              12
        a,c,b       v({a,c}) = 27       v({a,b,c}) = 36           9
        b,a,c       v({}) = 0           v({b}) = 6                6
        b,c,a       v({}) = 0           v({b}) = 6                6
        c,a,b       v({a,c}) = 27       v({a,b,c}) = 36           9
        c,b,a       v({c}) = 9          v({b,c}) = 15             6
-------------       --------------     --------------  --------------------
** Sh_b({a,b,c}, v ) = 8; analog f√ºr Sh_a = 17, Sh_c = 11

# Shapley value

###example1: 
### solve the regression problem
```{r}
#   task = bh.task,      predict.methods = regr.lm
s1 <-shapley(3, task = bh.task, model = train(makeLearner("regr.lm"), bh.task))
#Shapley Type
getShapleyTaskType(s1)
#Shapley predictionType
getShapleyPredictionType(s1)
# shapley Value
knitr::kable(getShapleyValues(s1))
```

###example2:
### solve the classification problem
```{r}
#  task = iris.task,      predict.methods = classif.lda
s2 <-shapley(3, task = iris.task, model = train(makeLearner("classif.lda"), iris.task))
#Shapley Type
getShapleyTaskType(s2)
#Shapley predictionType
getShapleyPredictionType(s2)
# shapley Value
knitr::kable(getShapleyValues(s2))
```

###example3:
### solve the multilabel problem
```{r}
#  task = yeast.task,      predict.methods = multilabel.rFerns
s3 <-shapley(3, task = yeast.task, model = train(makeLearner("multilabel.rFerns"), yeast.task))
#Shapley Type
getShapleyTaskType(s3)
#Shapley predictionType
getShapleyPredictionType(s3)
# shapley Value
knitr::kable(getShapleyValues(s3))
```

###example4:
### solve the cluster problem
```{r}
#  task = mtcars.task,      predict.methods = cluster.kmeans
s4 <-shapley(3, task = mtcars.task, model = train(makeLearner("cluster.kmeans"), mtcars.task))
#Shapley Type
getShapleyTaskType(s4)
#Shapley predictionType
getShapleyPredictionType(s4)
# shapley Value
knitr::kable(getShapleyValues(s4))
```





# Including Plots

This method draws a plot for, the observed value and describes the influence of features we interested .

###example1:
### show the singleVaule influence
```{r}
s1 <-shapley(3, task = bh.task, model = train(makeLearner("regr.lm"), bh.task))
plot.shapley.singleValue(s1)


```


###example3:
### show the multifeatures influence
```{r}
shap.values = shapley(1:100, task = bh.task, model = train(makeLearner("regr.lm"), bh.task))
plot.shapley.multipleFeatures(shap.values )

#This method draws a plot for, the observed value and describes the influence of multiple features we interested .
```

```{r, echo = FALSE}
#knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
```
# Convergence
Tests that the shapley algorithm converges
parameters are : 
    row.nr Index for the observation of interest.
    convergence.iterations Amount of iterations of the shapley function.
    iterations Amount of the iterations within the shapley
    function.
    return.value You can choose between plotting results or getting a data frame
return shapley value as a data.frame with col.names and their corresponding
effects.


```{r}

#plot the convergence
#test.convergence(return.value = "plot", ...)
#get a data frame
#test.convergence(return.value = "values")
#change amount of iterations in the shapley function
#test.convergence(iterations = ...)
#change amount of calls of shapey function
#test.convergence(convergence.iterations = ...)
#choose observation as reference, for example an observation that you know is normal
#test.convergence(row.nr = ...)

```
 

```{r, echo = TRUE}
#show the covergence value

test.convergence(row.nr=2, convergence.iterations = 20, iterations = 20, task = mtcars.task,
                            model = train(makeLearner("cluster.kmeans"), mtcars.task),
                            return.value = "values")
#The class, which is shown like this <<classname>>, is the class which was predicted by the model (also for clustering). Below the classname is shown how many times this class was chosen by the shapley function, because it has the biggest shapley value.

```



